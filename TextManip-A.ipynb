{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dela Rosa, Quielle Xyrone, 181603\n",
    "# Carl Alden Go, 182216\n",
    "# Margarita Juliana Perez, 183848\n",
    "\n",
    "# October 19, 2021\n",
    "\n",
    "# I certify that this submission complies with the DISCS Academic Integrity\n",
    "# Policy.\n",
    "\n",
    "# If I have discussed my Python language code with anyone other than\n",
    "# my instructor(s), my/our groupmate(s), the teaching assistant(s),\n",
    "# the extent of each discussion has been clearly noted along with a proper\n",
    "# citation in the comments of my program.\n",
    "\n",
    "# If any Python language code or documentation used in my program\n",
    "# was obtained from another source, either modified or unmodified, such as a\n",
    "# textbook, website, or another individual, the extent of its use has been\n",
    "# clearly noted along with a proper citation in the comments of my program.\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pickle-mixin in /Users/margauxperez/miniconda3/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: nltk in /Users/margauxperez/miniconda3/lib/python3.9/site-packages (3.6.3)\n",
      "Requirement already satisfied: sklearn in /Users/margauxperez/miniconda3/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: joblib in /Users/margauxperez/miniconda3/lib/python3.9/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /Users/margauxperez/miniconda3/lib/python3.9/site-packages (from nltk) (4.61.2)\n",
      "Requirement already satisfied: click in /Users/margauxperez/miniconda3/lib/python3.9/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: regex in /Users/margauxperez/miniconda3/lib/python3.9/site-packages (from nltk) (2021.9.24)\n",
      "Requirement already satisfied: scikit-learn in /Users/margauxperez/miniconda3/lib/python3.9/site-packages (from sklearn) (1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/margauxperez/miniconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.21.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/margauxperez/miniconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/margauxperez/miniconda3/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pickle-mixin nltk sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/margauxperez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pattern\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pattern.en import lemma, lexeme\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra filter/stop words in txt file\n",
    "\n",
    "#filer words are: character names, actor names\n",
    "with open('remove_words.txt') as file:\n",
    "    remove_words = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read our tweets from the previously created CSV\n",
    "column_names = [\"index\", \"date\", \"text\", \"handle\"]\n",
    "tweets_csv = pd.read_csv('out/tweets.csv', index_col=None, header=0, names=column_names)\n",
    "tweets_csv['text'].fillna('', inplace=True) #Remove blank texts\n",
    "tweets = tweets_csv.text.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning\n",
    "When cleaning our data, we want to remove unnecessary characters such as punctuations and whitespace. This is so that we can focus solely on the terms found in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\"\"\"\n",
    "Remove blank texts, replaces text with lower case characters,\n",
    "remove special characters, remove leading and trailing\n",
    "whitespaces, and remove stopwords.\n",
    "\"\"\"\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \n",
    "    if type(tweet) == np.float:\n",
    "        return \"\"\n",
    "    \n",
    "    temp = tweet.lower()\n",
    "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(r'http\\S+', '', temp)\n",
    "    temp = re.sub('[()!?]', ' ', temp)\n",
    "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
    "    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
    "    temp = re.sub(r'^\\s+|\\s+$', ' ', temp) #Remove trailing and leading whitespaces\n",
    "    temp = re.sub(r'[\\d-]+', ' ', temp) # Remove numerical chars / integers\n",
    "    temp = temp.split()\n",
    "    temp = [w for w in temp if not w in stopwords]\n",
    "    temp = [w for w in temp if not w in remove_words]\n",
    "    temp = [lemma(w) for w in temp] #Lemmatization\n",
    "    temp = \" \".join(word for word in temp)\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [clean_tweet(tw) for tw in tweets]\n",
    "dictionary = {'text': tweets, 'cleaned_text': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finished sex education season 3  didn't like t...</td>\n",
       "      <td>didnt like cliff hanger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Currently half way through season 3 of Sex edu...</td>\n",
       "      <td>currently half way absolutely clue addictive w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>season 3 of Sex Education has such a good soun...</td>\n",
       "      <td>good soundtrack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@JWRD__ - You season 3 , sex education season ...</td>\n",
       "      <td>film de slag om de schelde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>on episode 6 season 3 of sex education and i'v...</td>\n",
       "      <td>never want smack anyone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Finished sex education season 3  didn't like t...   \n",
       "1  Currently half way through season 3 of Sex edu...   \n",
       "3  season 3 of Sex Education has such a good soun...   \n",
       "4  @JWRD__ - You season 3 , sex education season ...   \n",
       "5  on episode 6 season 3 of sex education and i'v...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0                            didnt like cliff hanger  \n",
       "1  currently half way absolutely clue addictive w...  \n",
       "3                                    good soundtrack  \n",
       "4                         film de slag om de schelde  \n",
       "5                            never want smack anyone  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export the cleaned tweets into CSV\n",
    "cleaned_tweets = pd.DataFrame(dictionary)\n",
    "nan_value = float(\"NaN\")\n",
    "cleaned_tweets.replace(\"\", nan_value, inplace=True)\n",
    "cleaned_tweets.dropna(subset = [\"cleaned_text\"], inplace=True)\n",
    "cleaned_tweets.to_csv('out/cleaned_tweets.csv', index=False)\n",
    "cleaned_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text representation\n",
    "We also want to be able to transform our data from terms into numerals where we can apply quantitative techniques.\n",
    "\n",
    "1. **Document-term matrix**: occurence of words across documents\n",
    "2. **N-gram matrix**: occurence of n-grams (phrases of n length) accross documents\n",
    "3. **TFIDF matrix**: term frequency adjusted by the rarity of the in documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bi-gram (2 words in phrase)\n",
    "def tweets_to_ngram(tweets, n=2):\n",
    "    vectorizer = CountVectorizer(\n",
    "        ngram_range=(n, n),\n",
    "        token_pattern=r'\\b\\w+\\b',\n",
    "        min_df=1,\n",
    "        max_features=2000)\n",
    "    ngram = vectorizer.fit_transform(tweets)\n",
    "    pickle.dump(vectorizer, open('out/ngram.pk', 'wb'))\n",
    "    return ngram, vectorizer\n",
    "\n",
    "def tweets_to_tfidf(tweets):\n",
    "    vectorizer = TfidfVectorizer(max_features=2000)\n",
    "    tfidf = vectorizer.fit_transform(tweets)\n",
    "    pickle.dump(vectorizer, open('out/tfidf.pk', 'wb'))\n",
    "    return tfidf, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngram matrix shape: (1649, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Get bi-gram matrix\n",
    "ngram, ngram_v = tweets_to_ngram(cleaned_tweets['cleaned_text'], n=2)\n",
    "print('Ngram matrix shape:', ngram.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF matrix shape: (1649, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Get TFIDF matrix\n",
    "tfidf, tfidf_v = tweets_to_tfidf(cleaned_tweets['cleaned_text'])\n",
    "print('TFIDF matrix shape:', tfidf.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequencies\n",
    "We can convert our text metrices back into a list terms and their accompanying frequency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_frequency(vector, vectorizer):\n",
    "    \"\"\"\n",
    "    Return a list of words and their corresponding occurence in the corpus\n",
    "    \"\"\"\n",
    "    total = vector.sum(axis=0)\n",
    "    frequency = [(w, total[0, i]) for w, i in vectorizer.vocabulary_.items()]\n",
    "    frequency = pd.DataFrame(frequency, columns=['term', 'frequency'])\n",
    "    frequency = frequency.sort_values(by='frequency', ascending=False).reset_index(drop=True)\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inclusive intimacy</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>get inclusive</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>character development</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feel like</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cant wait</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>black queer</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>queer story</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>one best</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>best character</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>love show</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    term  frequency\n",
       "0     inclusive intimacy         25\n",
       "1          get inclusive         25\n",
       "2  character development         19\n",
       "3              feel like         18\n",
       "4              cant wait         14\n",
       "5            black queer         13\n",
       "6            queer story         13\n",
       "7               one best         11\n",
       "8         best character         10\n",
       "9              love show         10"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_ngram = vector_to_frequency(ngram, ngram_v)\n",
    "freq_ngram = freq_ngram.set_index('term')\n",
    "freq_ngram = freq_ngram.reset_index()\n",
    "freq_ngram.to_csv('out/frequency_ngram.csv', index=False)\n",
    "freq_ngram.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>44.053370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>get</td>\n",
       "      <td>40.419045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like</td>\n",
       "      <td>34.914889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>show</td>\n",
       "      <td>34.008885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love</td>\n",
       "      <td>33.027175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   term  frequency\n",
       "0  good  44.053370\n",
       "1   get  40.419045\n",
       "2  like  34.914889\n",
       "3  show  34.008885\n",
       "4  love  33.027175"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_tfidf = vector_to_frequency(tfidf, tfidf_v)\n",
    "freq_tfidf.to_csv('out/frequency_tfidf.csv', index=False)\n",
    "freq_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
